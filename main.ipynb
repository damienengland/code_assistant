{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56c0ecca-12f3-40f3-926d-6d4d1dd9779c",
   "metadata": {},
   "source": [
    "# Code Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc04ca8c-2e58-4f8f-b219-305ebf695fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "import ollama\n",
    "import gradio as gr\n",
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44caadc-1330-451a-af45-62e2ff9f5630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment variables\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46acca7e-23d4-4a80-8183-1f18e0b08077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to openai, google, anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()\n",
    "gemmini = genai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fe068f-e5a7-4a9b-84a4-bf6bd3830b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure ollama is loaded \n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afa63fd-d07c-48b2-b18b-11eb5117888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Models\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_OLLAMA = 'llama3.2'\n",
    "MODEL_CLAUDE = 'claude-3-haiku-20240307'\n",
    "MODEL_GEMINI = 'gemini-2.0-flash'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70526710-83ab-4bf6-b75e-c9b5e236acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define system prompt\n",
    "\n",
    "system_prompt = '''\n",
    "    You are AI assistant that helps explain what code does and why.\n",
    "    You should respond in Markdown.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d220d6b-d4ad-481c-837a-90dd93f0ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt call\n",
    "\n",
    "def message_gpt(question):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_prompt},\n",
    "        {'role': 'user', 'content': question}\n",
    "    ]\n",
    "    stream = openai.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=MODEL_GPT, \n",
    "        max_tokens=1000,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240873b9-fc34-4282-887d-bc26fe001842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama call\n",
    "\n",
    "def message_ollama(question):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_prompt},\n",
    "        {'role': 'user', 'content': question}\n",
    "    ]\n",
    "    stream = ollama.chat(\n",
    "        model=MODEL_OLLAMA,\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    result = ''\n",
    "    for chunk in stream:\n",
    "        result += chunk['message']['content'] or ''\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c4341e-e756-4b2b-b3a8-a2c2746b1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemini call\n",
    "\n",
    "def message_gemmini(question):\n",
    "    stream = gemmini.models.generate_content_stream(\n",
    "        model=MODEL_GEMINI,\n",
    "        config=types.GenerateContentConfig(system_instruction=system_prompt),\n",
    "        contents=[question]\n",
    "    )\n",
    "    result = ''\n",
    "    for chunk in stream:\n",
    "        result += chunk.text or ''\n",
    "        yield result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9931ec8e-dde0-4ddf-b532-a7d93cbf9382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# claude call\n",
    "\n",
    "def message_claude(question):\n",
    "    messages = [{'role': 'user', 'content': question}]\n",
    "    result = claude.messages.stream(\n",
    "        model=MODEL_CLAUDE,\n",
    "        max_tokens=1000,\n",
    "        system=system_prompt,\n",
    "        messages=messages\n",
    "    )\n",
    "    response = \"\"\n",
    "    with result as stream:\n",
    "        for text in stream.text_stream:\n",
    "            response += text or \"\"\n",
    "            yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a881d1a-2559-48b1-9a65-e9a486648fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_model(model, prompt):\n",
    "\n",
    "    if model == 'GPT':\n",
    "        result = message_gpt(prompt)\n",
    "    elif model == 'Claude':\n",
    "        result = message_claude(prompt)\n",
    "    elif model == 'Gemmini':\n",
    "        result = message_gemmini(prompt)\n",
    "    elif model == 'llama':\n",
    "        message_ollama(prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "\n",
    "    \n",
    "\n",
    "    yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fa361f-0a47-4922-b192-98c33703c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build UI with Gradio\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=stream_model,\n",
    "    inputs=[\n",
    "        gr.Dropdown(['GPT', 'Claude', 'Gemmini', 'llama'], label='Select Model:', value='GPT'),\n",
    "        gr.Textbox(label=\"Question:\")\n",
    "    ],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89555c85-c668-41e1-8ee4-f3a55e0be26c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
